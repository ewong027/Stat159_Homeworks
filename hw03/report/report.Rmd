---
title: "Simple Regression Analysis"
author: "Erica Wong"
date: "October 13, 2016"
output: 
  pdf_document:
    fig_caption: true
---

## Abstract
The purpose of this report is to apply the computational tools that we have learned about in class to reproduced a multiple regression analysis. Specifically, we are trying to reproduce the analysis that was done in section 3.2 of _An Introduction of Statistical Learning_ by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. [You can find a link to their book and data set by clicking on this sentence.](http://www-bcf.usc.edu/~gareth/ISL/) 

## Introduction
The goal of this study is to see how different types of advertising affects sales so that we can provide advice on what medium would be the best to improve sales of a specific product. In my report, I am specifcally looking to see if there is some association between TV, radio, newspaper, and sales. If there is an association between the response and explainatory variables, then I want to have a model that will be able to predict sales based on these 3 variables.

## Data
In this report, we are using data from [Advertising.csv](http://www-bcf.usc.edu/~gareth/ISL/Advertising.csv). In the data set we have four different columns. There is the TV advertising budget, radio advertising budget, and newspaper advertising budget. These budgets are given in thousands of dollars. Additionally, there is a sales column, which is given in thousands of units. In my report, we will be focusing on the TV and sales column of the Advertising.csv.

## Methodology
We look at the data set and study the relationship between TV, radio, newspaper, and sales. This statement helps us to set up our null and alternate hypothesis. The null hypothesis is $H_{0}$: $\beta_1 = \beta_2 = \cdots = \beta_p$. The alternative hypothesis is $H_{1}$: There exists at least one $\beta_j$ that does not equal 0. To test this, we want to use a multiple regression model, where the equation is $Sales = \beta_0 + \beta_1 TV + \beta_2 Radio + \beta_3 Newspaper + error$.In order to solve for $\beta_0$,  $\beta_1$, $\beta_2$, and $\beta_3$ we need to fit a linear model to our data, this will give us $\hat{\beta_0}$, $\hat{\beta_1}$, $\hat{\beta_2}$, and $\hat{\beta_3}$, which are estimates of $\beta_0$, $\beta_1$, $\beta_2$, and $\beta_3$. By solving for these objects,we get a line as close as possible to the 200 data points we have. In this report, we will fit a multiple linear regression model using the least squares criterion to our data. 

## Results
Here are the tables of the simple linear regression of sales vs each of the three variables:
This is for Simple regression of sales on TV.
```{r results= 'asis', echo = FALSE}
load('../data/regression.RData')
library(xtable)
tbl_TV <- xtable(regression_summary_TV,
              caption = 'Simple Regression of Sales on TV',
              digits = 4)
row.names(tbl_TV) <- c('(Intercept)','TV')

print(tbl_TV, caption.placement = 'top',
      comment = getOption("xtable.comment", FALSE))
```

This is for Simple regression of sales on radio.
```{r results= 'asis', echo = FALSE}
tbl_Radio <- xtable(regression_summary_Radio,
              caption = 'Simple Regression of Sales on Radio',
              digits = 4)
row.names(tbl_Radio) <- c('(Intercept)','radio')

print(tbl_Radio, caption.placement = 'top',
      comment = getOption("xtable.comment", FALSE))
```

This is for Simple regression of sales on newspaper.
```{r results= 'asis', echo = FALSE}
tbl_newspaper <- xtable(regression_summary_Newspaper,
              caption = 'Simple Regression of Sales on Newspaper',
              digits = 4)
row.names(tbl_newspaper) <- c('(Intercept)','newspaper')

print(tbl_newspaper, caption.placement = 'top',
      comment = getOption("xtable.comment", FALSE))
```


When we did multiple regression for our model, this these are the results that we obtained.
```{r results= 'asis', echo = FALSE}
tbl_multiple <- xtable(regression_summary_multiple,
              caption = 'Multiple Regression Table',
              digits = 4)
row.names(tbl_multiple) <- c('(Intercept)','TV','Radio','newspaper')

print(tbl_multiple, caption.placement = 'top',
      comment = getOption("xtable.comment", FALSE))
```

Additionally, we have included the correlation matrix between the different variables.
Correlation Matrix:
```{r results= 'asis', echo = FALSE}
load('../data/correlation-matrix.RData')
correlation_matrix <- round(correlation_matrix, digits = 4)
correlation_matrix[lower.tri(correlation_matrix)] <- ''
cor_matrix <- xtable(correlation_matrix,
              caption = 'Correlation Matrix',
              digits = 4)
print(cor_matrix, caption.placement = 'top',
      comment = getOption("xtable.comment", FALSE))
```

In this report, we want to answer 4 main questions. Those questions are:

1. Is at least one of the predictors usefil in predicting the response?
2. Do all predictors help to explain the response, or is it only a subset of the predictors useful?
3. How well does the model fit the data?
4. How accurate is the prediction?

To answer these questions, we will be using the informations from the tables produced above (in addition to other tables) and draw conclusions from those.

###1. Is at least one of the predictors useful in predicting the response?
When asking this question, we are setting up the null hypothesis to be: $H_{0}$: $\beta_1 = \beta_2 = \cdots = \beta_p$. The alternative hypothesis in this case is $H_{1}$: There exists at least one $\beta_j$ that does not equal 0. In order to see if there is a relationship we can look at the F-statistic. If there is no relationship between the predictors and the response variable, in our case TV, radio, and newspaper with sales, then the F-statistic will be close to 1. However, if the F-statistic is greater than 1, then there is probably a relationship between the predictors and the response and we will reject the null hypothesis. Here is a chart containing additional information, and one of the things that is shown in this table is the F-statistic.

```{r results= 'asis', echo = FALSE}
Quantity <- c('RSS', 'R2', 'F-stat')
Value <- c(regression_summary_multiple$sigma,
           regression_summary_multiple$adj.r.squared,
           regression_summary_multiple$fstatistic[1])
more_info <- data.frame(Quantity, Value)
tbl <- xtable(more_info,
              caption = 'More Info on Multiple Least Squares Model',
              digits = 3)

print(tbl, caption.placement = 'top',
      comment = getOption("xtable.comment", FALSE),
      include.rownames = FALSE)
```

From this table, we can see that the F-statistic = `r round(regression_summary_multiple$fstatistic[[1]], digits = 3)`. This is a very large number, which is much greater than 1. Thus, we would reject the null hypothesis due to the large number and we have enough reason to believe that there is at least one predictor variable has a relationship with the response variables. 

###2. Do all predictors help to explain the response, or is it only a subset of the predictors useful?

When deciding on which predictors help to explain the reponse, we can look at the p-values. When looking at the p-value, if the value is small, then there is reason to believe that the predictors help to explain the response. However, if the p-value is large, then we have reason to believe that there is no relationship between that predictor and the response variable. The p-value (rounded to the thousandth) of TV in relation to sales is `r round(regression_summary_multiple$coefficients[2,4], digits = 3)` and the p-value of radio (rounded to the thousandth) is `r round(regression_summary_multiple$coefficients[3,4], digits = 3)`. Both of these p-values are fairly small, so there is good reason to believe that these predictors help to explain the reponse variable. However, the p-value of newspaper is `r regression_summary_multiple$coefficients[4,4]`. This is a very large p-value, so there is reason to believe that there is no relationship between newspaper budget and sales. 

###3. How well does the model fit the data?
To know how well our model fits the data, we can look at the RSE and R^2. RSE stands for residual standard error. RSE is the estimate of the standard deviation of error. It can also be described as the lack of fit in the model. So, when RSE is small, that means that the model fits the data well. In this multiple regression model, the RSE = `r round(regression_summary_multiple$sigma, digits = 3)`. Since RSE is measured in absolute terms, we can tell that our RSE is small enough to say that this is a good fit.

We can also look at $R^{2}$, which tells us how much of the dependent variable can be explained by the independent variables. $R^{2}$ is the square of the correlation of the response and the variable, so the closer that is to 1, the better our model fits the data. $R^{2}$ can only take on values between 0 and 1. Looking at the model that uses all 3 variables, our $R^{2}$ = `r round(regression_summary_multiple$r.squared, digits = 3)`. This is very close to 1. So, that means that a lot of our response variable, sales, can be explained by the explainatory variables. 

So, looking at these 2 criteria, we can see that our multiple regression model with all 3 variables fits very well overall.

###4. How accurate is the prediction?
As explained in the previous question, we know that our prediction is fairly accurate because our $R^{2}$  is fairly high. So, this contributes to how accurate our model is. Also, we can look at the stanard deviations of each of our explainatory variables. The standard deviation related to each of predictor variables are all fairly small For TV, radio, and newspaper, they are `r regression_summary_multiple$coefficients[2,2]`, `r regression_summary_multiple$coefficients[2,3]`, approximately`r round(regression_summary_multiple$coefficients[2,4], digits = 3)`. respectively. So, that means that our predictions of $\beta_1$, $\beta_2$, and $\beta_3$ are very close to the true model and thus we have an accurate fitted model so the predictions are fairly accurate.

## Conclusion
So, in this project, we were able to recreate the study that was done in chapter 3.2 of _An Introduction of Statistical Learning_ by Gareth James, Daniela Witten, Trevor Hastie and Robert Tibshirani. We also found that there is some relationship between TV, radio, and newspaper budget with sales. This is shown through the rejection of our null hypothesis. Our null hypothesis is $H_{0}$: $\beta_1 = \beta_2 = \cdots = \beta_p$. Additionally, we know that our model fits well because $R^{2}$ is close to 1 and RSE is small by comparative standards. 
